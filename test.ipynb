{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37fdfb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from tool call name:  get_weather with arguments:  {'city': 'London'} result:  The temperature in London is 9Â°C\n",
      "\n",
      "Result from tool call name:  get_weather with arguments:  {'city': 'Toronto'} result:  The temperature in Toronto is 23Â°C\n",
      "\n",
      "Content: \n",
      "Okay, I have the weather in London and Toronto. \n",
      "\n",
      "London: 9Â°C\n",
      "Toronto: 23Â°C\n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# from rich import print\n",
    "\n",
    "from ollama import Client\n",
    "from ollama._types import ChatResponse\n",
    "import ollama\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "  \"\"\"\n",
    "  Get the current temperature for a city\n",
    "\n",
    "  Args:\n",
    "      city (str): The name of the city\n",
    "\n",
    "  Returns:\n",
    "      str: The current temperature\n",
    "  \"\"\"\n",
    "  temperatures = list(range(-10, 35))\n",
    "\n",
    "  temp = random.choice(temperatures)\n",
    "\n",
    "  return f'The temperature in {city} is {temp}Â°C'\n",
    "\n",
    "\n",
    "def get_weather_conditions(city: str) -> str:\n",
    "  \"\"\"\n",
    "  Get the weather conditions for a city\n",
    "\n",
    "  Args:\n",
    "      city (str): The name of the city\n",
    "\n",
    "  Returns:\n",
    "      str: The current weather conditions\n",
    "  \"\"\"\n",
    "  conditions = ['sunny', 'cloudy', 'rainy', 'snowy', 'foggy']\n",
    "  return random.choice(conditions)\n",
    "\n",
    "\n",
    "available_tools = {'get_weather': get_weather, 'get_weather_conditions': get_weather_conditions}\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What is the weather like in London? What are the conditions in Toronto?'}]\n",
    "\n",
    "\n",
    "# client = Client(\n",
    "#   # Ollama Turbo\n",
    "#   # host=\"https://ollama.com\", headers={'Authorization': (os.getenv('OLLAMA_API_KEY'))}\n",
    "# )\n",
    "# model = 'gpt-oss:20b'\n",
    "model = 'gemma3n-tools'\n",
    "# gpt-oss can call tools while \"thinking\"\n",
    "# a loop is needed to call the tools and get the results\n",
    "while True:\n",
    "  response: ChatResponse = ollama.chat(model=model, messages=messages, tools=[get_weather, get_weather_conditions])\n",
    "\n",
    "  if response.message.content:\n",
    "    print('Content: ')\n",
    "    print(response.message.content + '\\n')\n",
    "  if response.message.thinking:\n",
    "    print('Thinking: ')\n",
    "    print(response.message.thinking + '\\n')\n",
    "\n",
    "  messages.append(response.message)\n",
    "\n",
    "  if response.message.tool_calls:\n",
    "    for tool_call in response.message.tool_calls:\n",
    "      function_to_call = available_tools.get(tool_call.function.name)\n",
    "      if function_to_call:\n",
    "        result = function_to_call(**tool_call.function.arguments)\n",
    "        print('Result from tool call name: ', tool_call.function.name, 'with arguments: ', tool_call.function.arguments, 'result: ', result + '\\n')\n",
    "        messages.append({'role': 'tool', 'content': result, 'tool_name': tool_call.function.name})\n",
    "      else:\n",
    "        print(f'Tool {tool_call.function.name} not found')\n",
    "        messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})\n",
    "  else:\n",
    "    # no more tool calls, we can stop the loop\n",
    "    break\n",
    "    # Get user input and add to messages\n",
    "    # user_input = input(\">\")\n",
    "    # messages.append({'role': 'user', 'content': user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2622aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.get_weather(city: str) -> str>,\n",
       " <function __main__.get_weather_conditions(city: str) -> str>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools=[get_weather, get_weather_conditions]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ba78da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENROUTER_API_BASE\"] = \"\" # [OPTIONAL] defaults to https://openrouter.ai/api/v1\n",
    "\n",
    "messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    "response = completion(\n",
    "            model=\"openrouter/qwen/qwen3-14b:free\",\n",
    "            messages=messages,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc189b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='gen-1757135038-tSAcu3UGbpEO5RkGUJC9', created=1757135038, model='qwen/qwen3-14b:free', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm just a machine, so I don't have feelings, but I'm here and ready to help you with whatever you need! ðŸ˜Š How are you doing today? Let me know if there's anything I can assist you with!\", role='assistant', tool_calls=None, function_call=None, reasoning_content='\\nOkay, the user greeted me with \"Hello, how are you?\" I should respond politely and warmly. Let me start with a friendly greeting. Since I\\'m an AI, I don\\'t have feelings, but I can express a positive attitude. I should mention that I\\'m just a machine and don\\'t experience emotions, but I\\'m here to help. Then, I need to ask them how they\\'re doing to keep the conversation going. Make sure the tone is cheerful and welcoming. Maybe add an emoji to make it more friendly. Check for any grammar issues. Alright, that should cover it.\\n', provider_specific_fields={'refusal': None, 'reasoning': '\\nOkay, the user greeted me with \"Hello, how are you?\" I should respond politely and warmly. Let me start with a friendly greeting. Since I\\'m an AI, I don\\'t have feelings, but I can express a positive attitude. I should mention that I\\'m just a machine and don\\'t experience emotions, but I\\'m here to help. Then, I need to ask them how they\\'re doing to keep the conversation going. Make sure the tone is cheerful and welcoming. Maybe add an emoji to make it more friendly. Check for any grammar issues. Alright, that should cover it.\\n', 'reasoning_content': '\\nOkay, the user greeted me with \"Hello, how are you?\" I should respond politely and warmly. Let me start with a friendly greeting. Since I\\'m an AI, I don\\'t have feelings, but I can express a positive attitude. I should mention that I\\'m just a machine and don\\'t experience emotions, but I\\'m here to help. Then, I need to ask them how they\\'re doing to keep the conversation going. Make sure the tone is cheerful and welcoming. Maybe add an emoji to make it more friendly. Check for any grammar issues. Alright, that should cover it.\\n'}), provider_specific_fields={'native_finish_reason': 'stop'})], usage=Usage(completion_tokens=174, prompt_tokens=15, total_tokens=189, completion_tokens_details=None, prompt_tokens_details=None), provider='Chutes')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "736e7c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ollama_chat/qwen3:8b': {'supports_function_calling': True}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model=\"openrouter/qwen/qwen3-30b-a3b:free\"\n",
    "model=\"ollama/qwen3:8b\"\n",
    "import litellm\n",
    "litellm.register_model(model_cost={\n",
    "                \"ollama_chat/qwen3:8b\": { \n",
    "                  \"supports_function_calling\": True\n",
    "                },\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4031ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"openrouter/qwen/qwen3-4b:free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63ffce8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openrouter/qwen/qwen3-30b-a3b:free'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95b631d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'function', 'function': {'name': 'get_weather', 'description': 'Get the current temperature for a city', 'parameters': {'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'Parameter: city'}}, 'required': ['city']}}}, {'type': 'function', 'function': {'name': 'get_weather_conditions', 'description': 'Get the weather conditions for a city', 'parameters': {'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'Parameter: city'}}, 'required': ['city']}}}]\n",
      "Choices(finish_reason='tool_calls', index=0, message=Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"city\": \"London\"}', name='get_weather'), id='chatcmpl-tool-bda756d15cde425b826e32c5882cf41e', type='function'), ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"city\": \"Toronto\"}', name='get_weather_conditions'), id='chatcmpl-tool-0e6c9620cf614126ae7f212d3fc09c56', type='function')], function_call=None, reasoning_content='Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n', provider_specific_fields={'refusal': None, 'reasoning': 'Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n', 'reasoning_content': 'Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n'}), provider_specific_fields={'native_finish_reason': 'tool_calls'})\n",
      "Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"city\": \"London\"}', name='get_weather'), id='chatcmpl-tool-bda756d15cde425b826e32c5882cf41e', type='function'), ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"city\": \"Toronto\"}', name='get_weather_conditions'), id='chatcmpl-tool-0e6c9620cf614126ae7f212d3fc09c56', type='function')], function_call=None, reasoning_content='Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n', provider_specific_fields={'refusal': None, 'reasoning': 'Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n', 'reasoning_content': 'Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n'})\n",
      "Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"city\": \"London\"}', name='get_weather'), id='chatcmpl-tool-bda756d15cde425b826e32c5882cf41e', type='function'), ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"city\": \"Toronto\"}', name='get_weather_conditions'), id='chatcmpl-tool-0e6c9620cf614126ae7f212d3fc09c56', type='function')], function_call=None, reasoning_content='Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n', provider_specific_fields={'refusal': None, 'reasoning': 'Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n', 'reasoning_content': 'Okay, the user is asking about the weather in London and the conditions in Toronto. Let me see. They mentioned two different cities, so I need to check each one. The available functions are get_weather and get_weather_conditions. Wait, the first function is get_weather which gives the temperature, and the second is get_weather_conditions for the conditions. But the user asked for \"what is the weather like\" in London, which might refer to the temperature, and \"what are the conditions in Toronto,\" which would be the conditions. So I should call get_weather for London and get_weather_conditions for Toronto. Let me make sure the parameters are correct. The city names are London and Toronto. So I need to make two separate function calls. The user didn\\'t specify any other parameters, so just the city names. Alright, that\\'s straightforward. I\\'ll structure the tool calls accordingly.\\n'})\n",
      "Result from tool call name:  get_weather with arguments:  {'city': 'London'} result:  The temperature in London is -5Â°C\n",
      "\n",
      "Result from tool call name:  get_weather_conditions with arguments:  {'city': 'Toronto'} result:  rainy\n",
      "\n",
      "Choices(finish_reason='stop', index=0, message=Message(content='The temperature in London is -5Â°C.  \\nThe conditions in Toronto are rainy.', role='assistant', tool_calls=None, function_call=None, reasoning_content='Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n', provider_specific_fields={'refusal': None, 'reasoning': 'Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n', 'reasoning_content': 'Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n'}), provider_specific_fields={'native_finish_reason': 'stop'})\n",
      "Message(content='The temperature in London is -5Â°C.  \\nThe conditions in Toronto are rainy.', role='assistant', tool_calls=None, function_call=None, reasoning_content='Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n', provider_specific_fields={'refusal': None, 'reasoning': 'Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n', 'reasoning_content': 'Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n'})\n",
      "Content: \n",
      "The temperature in London is -5Â°C.  \n",
      "The conditions in Toronto are rainy.\n",
      "\n",
      "Message(content='The temperature in London is -5Â°C.  \\nThe conditions in Toronto are rainy.', role='assistant', tool_calls=None, function_call=None, reasoning_content='Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n', provider_specific_fields={'refusal': None, 'reasoning': 'Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n', 'reasoning_content': 'Okay, the user asked about the weather in London and the conditions in Toronto. I need to provide both answers. First, I called the get_weather function for London and got a temperature of -5Â°C. Then, I used get_weather_conditions for Toronto and received \"rainy\" as the response.\\n\\nNow, I should format the answers clearly. For London, I\\'ll state the temperature. For Toronto, mention that the conditions are rainy. Make sure to present them as separate responses. Check if there\\'s any additional info needed, but since the user only asked for the weather and conditions, this should suffice. Avoid any markdown and keep it straightforward.\\n'})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from pprint import pprint\n",
    "from litellm import completion\n",
    "from tools import create_tools_from_functions\n",
    "def get_weather(city: str) -> str:\n",
    "  \"\"\"\n",
    "  Get the current temperature for a city\n",
    "\n",
    "  Args:\n",
    "      city (str): The name of the city\n",
    "\n",
    "  Returns:\n",
    "      str: The current temperature\n",
    "  \"\"\"\n",
    "  temperatures = list(range(-10, 35))\n",
    "\n",
    "  temp = random.choice(temperatures)\n",
    "\n",
    "  return f'The temperature in {city} is {temp}Â°C'\n",
    "\n",
    "\n",
    "def get_weather_conditions(city: str) -> str:\n",
    "  \"\"\"\n",
    "  Get the weather conditions for a city\n",
    "\n",
    "  Args:\n",
    "      city (str): The name of the city\n",
    "\n",
    "  Returns:\n",
    "      str: The current weather conditions\n",
    "  \"\"\"\n",
    "  conditions = ['sunny', 'cloudy', 'rainy', 'snowy', 'foggy']\n",
    "  return random.choice(conditions)\n",
    "\n",
    "\n",
    "available_tools = {'get_weather': get_weather, 'get_weather_conditions': get_weather_conditions}\n",
    "tools = create_tools_from_functions(available_tools)\n",
    "messages = [{'role': 'user', 'content': 'What is the weather like in London? What are the conditions in Toronto?'}]\n",
    "print (tools)\n",
    "# client = Client(\n",
    "#   # Ollama Turbo\n",
    "#   # host=\"https://ollama.com\", headers={'Authorization': (os.getenv('OLLAMA_API_KEY'))}\n",
    "# )\n",
    "# model = 'gpt-oss:20b'\n",
    "# model = 'gemma3n-tools'\n",
    "# gpt-oss can call tools while \"thinking\"\n",
    "# a loop is needed to call the tools and get the results\n",
    "while True:\n",
    "  response = completion(model=model, messages=messages, tools=tools)\n",
    "  # pprint(response.choices[0].reasoning)\n",
    "  pprint(response.choices[0])\n",
    "  pprint(response.choices[0].message)\n",
    "  \n",
    "  # Access the message correctly\n",
    "  message = response.choices[0].message\n",
    "  \n",
    "  if message.content:\n",
    "    print('Content: ')\n",
    "    print(message.content + '\\n')\n",
    "  if hasattr(message, 'thinking') and message.thinking:\n",
    "    print('Thinking: ')\n",
    "    print(message.thinking + '\\n')\n",
    "\n",
    "  messages.append(message)\n",
    "  print(message)\n",
    "  # Check for tool calls correctly\n",
    "  if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "      function_to_call = available_tools.get(tool_call.function.name)\n",
    "      if function_to_call:\n",
    "        try:\n",
    "          # Parse the JSON arguments string\n",
    "          arguments = json.loads(tool_call.function.arguments)\n",
    "          result = function_to_call(**arguments)\n",
    "          print('Result from tool call name: ', tool_call.function.name, 'with arguments: ', arguments, 'result: ', result + '\\n')\n",
    "          # Use tool role with tool_call_id as per OpenRouter documentation\n",
    "          messages.append({\n",
    "            'role': 'tool', \n",
    "            'content': result,\n",
    "            'tool_call_id': tool_call.id\n",
    "          })\n",
    "        except (json.JSONDecodeError, TypeError) as e:\n",
    "          print(f'Error parsing arguments for {tool_call.function.name}: {e}')\n",
    "          messages.append({\n",
    "            'role': 'tool', \n",
    "            'content': f\"Error parsing arguments: {e}\",\n",
    "            'tool_call_id': tool_call.id\n",
    "          })\n",
    "        except Exception as e:\n",
    "          print(f'Error calling {tool_call.function.name}: {e}')\n",
    "          messages.append({\n",
    "            'role': 'tool', \n",
    "            'content': f\"Error calling function: {e}\",\n",
    "            'tool_call_id': tool_call.id\n",
    "          })\n",
    "      else:\n",
    "        print(f'Tool {tool_call.function.name} not found')\n",
    "        messages.append({\n",
    "          'role': 'tool', \n",
    "          'content': f\"Tool {tool_call.function.name} not found\",\n",
    "          'tool_call_id': tool_call.id\n",
    "        })\n",
    "  else:\n",
    "    # no more tool calls, we can stop the loop\n",
    "    break\n",
    "    # Get user input and add to messages\n",
    "    # user_input = input(\">\")\n",
    "    # messages.append({'role': 'user', 'content': user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13c8bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'What is the weather like in London? What are the conditions in '\n",
      "             'Toronto?',\n",
      "  'role': 'user'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather'}, 'id': 'call_68b2efc1-b876-4d00-b51c-c4f45f3650f0', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'The temperature in London is 16Â°C',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_68b2efc1-b876-4d00-b51c-c4f45f3650f0'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather'}, 'id': 'call_03478066-ef7e-4062-bfd1-02ad07494b74', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'The temperature in London is 16Â°C',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_03478066-ef7e-4062-bfd1-02ad07494b74'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather'}, 'id': 'call_b666cabd-7399-4f6e-835e-14bb876f569b', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'The temperature in London is 3Â°C',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_b666cabd-7399-4f6e-835e-14bb876f569b'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather'}, 'id': 'call_dbf8ba00-2d89-4ced-8dc2-836fb474c7d4', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'The temperature in London is 26Â°C',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_dbf8ba00-2d89-4ced-8dc2-836fb474c7d4'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather'}, 'id': 'call_c55d68ac-50f2-4cbd-99fe-db5fd2d6992b', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'The temperature in London is -5Â°C',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_c55d68ac-50f2-4cbd-99fe-db5fd2d6992b'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"Toronto\"}', 'name': 'get_weather_conditions'}, 'id': 'call_a071a1a8-af66-44ff-922b-f03fb3a9ab56', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'sunny',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_a071a1a8-af66-44ff-922b-f03fb3a9ab56'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather_conditions'}, 'id': 'call_7ccbe580-5845-4861-9ab7-559e0a84e7d2', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'rainy',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_7ccbe580-5845-4861-9ab7-559e0a84e7d2'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather'}, 'id': 'call_97fa19a8-114d-4aed-b676-756996a6c63a', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'The temperature in London is 16Â°C',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_97fa19a8-114d-4aed-b676-756996a6c63a'},\n",
      " Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"city\": \"London\"}', 'name': 'get_weather'}, 'id': 'call_25869f06-3369-4203-953c-98ae081c4e40', 'type': 'function'}], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'The temperature in London is 24Â°C',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_25869f06-3369-4203-953c-98ae081c4e40'},\n",
      " Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"city\": \"London\"}', name='get_weather_conditions'), id='call_6eae61ac-9fa9-4977-855d-abc4fce0ada8', type='function')], function_call=None, provider_specific_fields=None),\n",
      " {'content': 'foggy',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_6eae61ac-9fa9-4977-855d-abc4fce0ada8'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa935751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "print (litellm.supports_function_calling(model=\"openrouter/qwen/qwen3-14b:free\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "619a279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Here's the weather information:\\n\"\n",
      " '\\n'\n",
      " '**London**  \\n'\n",
      " '- **Temperature:** 32Â°C  \\n'\n",
      " '- **Conditions:** Cloudy  \\n'\n",
      " '\\n'\n",
      " '**Toronto**  \\n'\n",
      " '- **Conditions:** Cloudy  \\n'\n",
      " '\\n'\n",
      " \"Note: Toronto's temperature was not provided in the tool response. Let me \"\n",
      " \"know if you'd like additional details!\")\n"
     ]
    }
   ],
   "source": [
    "pprint  (messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc165f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "client.chat(\n",
      "    model: str = \u001b[33m''\u001b[39m,\n",
      "    messages: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Message]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    *,\n",
      "    tools: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Tool, Callable]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    stream: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    think: Union[bool, Literal[\u001b[33m'low'\u001b[39m, \u001b[33m'medium'\u001b[39m, \u001b[33m'high'\u001b[39m], NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    format: Union[Literal[\u001b[33m''\u001b[39m, \u001b[33m'json'\u001b[39m], dict[str, Any], NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    keep_alive: Union[float, str, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> Union[ollama._types.ChatResponse, collections.abc.Iterator[ollama._types.ChatResponse]]\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "  \u001b[38;5;28;01mdef\u001b[39;00m chat(\n",
      "    self,\n",
      "    model: str = \u001b[33m''\u001b[39m,\n",
      "    messages: Optional[Sequence[Union[Mapping[str, Any], Message]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    *,\n",
      "    tools: Optional[Sequence[Union[Mapping[str, Any], Tool, Callable]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    stream: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    think: Optional[Union[bool, Literal[\u001b[33m'low'\u001b[39m, \u001b[33m'medium'\u001b[39m, \u001b[33m'high'\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    format: Optional[Union[Literal[\u001b[33m''\u001b[39m, \u001b[33m'json'\u001b[39m], JsonSchemaValue]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    options: Optional[Union[Mapping[str, Any], Options]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    keep_alive: Optional[Union[float, str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "  ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n",
      "    \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m    Create a chat response using the requested model.\u001b[39m\n",
      "\n",
      "\u001b[33m    Args:\u001b[39m\n",
      "\u001b[33m      tools:\u001b[39m\n",
      "\u001b[33m        A JSON schema as a dict, an Ollama Tool or a Python Function.\u001b[39m\n",
      "\u001b[33m        Python functions need to follow Google style docstrings to be converted to an Ollama Tool.\u001b[39m\n",
      "\u001b[33m        For more information, see: https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\u001b[39m\n",
      "\u001b[33m      stream: Whether to stream the response.\u001b[39m\n",
      "\u001b[33m      format: The format of the response.\u001b[39m\n",
      "\n",
      "\u001b[33m    Example:\u001b[39m\n",
      "\u001b[33m      def add_two_numbers(a: int, b: int) -> int:\u001b[39m\n",
      "\u001b[33m        '''\u001b[39m\n",
      "\u001b[33m        Add two numbers together.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m          a: First number to add\u001b[39m\n",
      "\u001b[33m          b: Second number to add\u001b[39m\n",
      "\n",
      "\u001b[33m        Returns:\u001b[39m\n",
      "\u001b[33m          int: The sum of a and b\u001b[39m\n",
      "\u001b[33m        '''\u001b[39m\n",
      "\u001b[33m        return a + b\u001b[39m\n",
      "\n",
      "\u001b[33m      client.chat(model='llama3.2', tools=[add_two_numbers], messages=[...])\u001b[39m\n",
      "\n",
      "\u001b[33m    Raises `RequestError` if a model is not provided.\u001b[39m\n",
      "\n",
      "\u001b[33m    Raises `ResponseError` if the request could not be fulfilled.\u001b[39m\n",
      "\n",
      "\u001b[33m    Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m self._request(\n",
      "      ChatResponse,\n",
      "      \u001b[33m'POST'\u001b[39m,\n",
      "      \u001b[33m'/api/chat'\u001b[39m,\n",
      "      json=ChatRequest(\n",
      "        model=model,\n",
      "        messages=list(_copy_messages(messages)),\n",
      "        tools=list(_copy_tools(tools)),\n",
      "        stream=stream,\n",
      "        think=think,\n",
      "        format=format,\n",
      "        options=options,\n",
      "        keep_alive=keep_alive,\n",
      "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
      "      stream=stream,\n",
      "    )\n",
      "\u001b[31mFile:\u001b[39m      ~/Documents/gen-ai/crush-demo/coding-agent/venv/lib/python3.12/site-packages/ollama/_client.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client(\n",
    "  # Ollama Turbo\n",
    "  host=\"https://ollama-host.loca.lt/\", \n",
    "  # headers={'Authorization': (os.getenv('OLLAMA_API_KEY'))}\n",
    ")\n",
    "\n",
    "client.chat??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5850870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
